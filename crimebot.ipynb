{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPl8B/K8nnjb84XekcTPO9V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Amanmeena2004/chatbot-/blob/main/crimebot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai==0.28"
      ],
      "metadata": {
        "collapsed": true,
        "id": "kw5_mIxTlVG2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8131d86f-d467-4ea0-ebdd-b0a131bea679"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.9.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.7.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!openai migrate"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JCnpKGCak9Lr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9722f8f0-dda4-4869-b94b-a0c32ebd1501"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: openai [-h] [-V] [-v] [-b API_BASE] [-k API_KEY] [-p PROXY [PROXY ...]] [-o ORGANIZATION]\n",
            "              {api,tools,wandb} ...\n",
            "openai: error: argument {api,tools,wandb}: invalid choice: 'migrate' (choose from 'api', 'tools', 'wandb')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "I84_Deq2Zdml",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "370d42f6-a036-4155-ebfb-0f0d4469bb11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg9IPj7qYnyF"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import openai\n",
        "import string\n",
        "import random\n",
        "import time\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "openai.api_key = 'sk-None-y311uYNkT17vlwOGjkyRT3BlbkFJfIXR5O9pHzMKlhDj6f2i'"
      ],
      "metadata": {
        "id": "oFR_yCHJifK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the statistical data"
      ],
      "metadata": {
        "id": "lLVJopxRuWEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_csv_with_error_handling(file_path):\n",
        "    try:\n",
        "        return pd.read_csv(file_path)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "ipc_crimes_df = read_csv_with_error_handling('/content/IPC CRIMES 2019.csv')\n",
        "crime_foreigners_2022_df = read_csv_with_error_handling('/content/crimeheldwithforeigners2022.csv')\n",
        "crime_foreigners_2017_2021_df = read_csv_with_error_handling('/content/crimesheldwithforeigners2017-2021.csv')\n",
        "\n",
        "if ipc_crimes_df is not None and crime_foreigners_2022_df is not None and crime_foreigners_2017_2021_df is not None:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "    # Function to concatenate text from multiple columns\n",
        "    def concatenate_columns(df, columns):\n",
        "        return df[columns].astype(str).agg(' '.join, axis=1).tolist()\n",
        "\n",
        "    # Combine text from specific columns if needed\n",
        "    ipc_crimes_text = concatenate_columns(ipc_crimes_df, ['column1', 'column2'])\n",
        "    crime_foreigners_2022_text = concatenate_columns(crime_foreigners_2022_df, ['column1', 'column2'])\n",
        "    crime_foreigners_2017_2021_text = concatenate_columns(crime_foreigners_2017_2021_df, ['column1', 'column2'])\n",
        "\n",
        "    # Combine all text data into a single list\n",
        "    all_text = ipc_crimes_text + crime_foreigners_2022_text + crime_foreigners_2017_2021_text\n",
        "\n",
        "    sent_tokens = nltk.sent_tokenize(\" \".join(all_text))\n",
        "    word_tokens = nltk.word_tokenize(\" \".join(all_text))\n",
        "\n",
        "    print(f\"Sentences: {sent_tokens[:5]}\")\n",
        "    print(f\"Words: {word_tokens[:5]}\")\n",
        "else:\n",
        "    print(\"One or more CSV files could not be loaded.\")"
      ],
      "metadata": {
        "id": "6I4BBQtrl4nn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09e34974-3c92-40ff-a10f-c58fc9f7be70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: [Errno 2] No such file or directory: '/content/IPC CRIMES 2019.csv'\n",
            "Error: [Errno 2] No such file or directory: '/content/crimeheldwithforeigners2022.csv'\n",
            "Error: [Errno 2] No such file or directory: '/content/crimesheldwithforeigners2017-2021.csv'\n",
            "One or more CSV files could not be loaded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Combine text from all the data files into a single raw string for tokenization"
      ],
      "metadata": {
        "id": "BofNmtt9u387"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "def LemTokens(tokens):\n",
        "    return [lemmer.lemmatize(token) for token in tokens]\n",
        "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
        "def LemNormalize(text):\n",
        "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "LfAEB6yHeBDr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GREET_INPUT = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\",\"hey\")\n",
        "GREET_RESPONSE = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
        "def greet(sentence):\n",
        "    for word in sentence.split():\n",
        "        if word.lower() in GREET_INPUT:\n",
        "            return random.choice(GREET_RESPONSE)"
      ],
      "metadata": {
        "id": "W6zMmwyieSKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and train a logistic regression model"
      ],
      "metadata": {
        "id": "LipncbanznRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(sent_tokens):\n",
        "    # Create a DataFrame with the input text data and dummy labels\n",
        "    data = {\n",
        "        'text': sent_tokens,\n",
        "        'label': [random.choice([0, 1]) for _ in range(len(sent_tokens))]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Convert text to TF-IDF features\n",
        "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words='english')\n",
        "    X = TfidfVec.fit_transform(df['text'])\n",
        "    y = df['label']\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)\n",
        "\n",
        "    # Train the model\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Test the model\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\n",
        "\n",
        "    return model, TfidfVec\n",
        "\n",
        "model, TfidfVec = train_model(sent_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "FY69VQ7NeaIj",
        "outputId": "d334a949-3e51-4b88-9bb4-d95a7c891f12"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'sent_tokens' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-eac19fdabda8>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfidfVec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTfidfVec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'sent_tokens' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_response):\n",
        "    robo1_response = ''\n",
        "    tfidf = TfidfVec.transform([user_response])\n",
        "    prediction = model.predict(tfidf)[0]\n",
        "    if prediction == 0:\n",
        "        robo1_response = \"I am sorry! I don't understand you\"\n",
        "    else:\n",
        "        vals = cosine_similarity(tfidf, TfidfVec.transform(sent_tokens))\n",
        "        idx = vals.argsort()[0][-2]\n",
        "        robo1_response = sent_tokens[idx]\n",
        "    return robo1_response"
      ],
      "metadata": {
        "id": "f5KP9CqYeZLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chatgpt_response(prompt, max_retries=3, retry_delay=5):\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            response = openai.ChatCompletion.create(\n",
        "                model=\"gpt-3.5-turbo\",\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                max_tokens=150,\n",
        "                n=1,\n",
        "                stop=None,\n",
        "                temperature=0.7,\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "        except openai.error.RateLimitError:\n",
        "            retries += 1\n",
        "            print(f\"Rate limit exceeded. Retrying in {retry_delay} seconds... (Attempt {retries})\")\n",
        "            time.sleep(retry_delay)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred: {e}\")\n",
        "            break\n",
        "    print(\"Max retries reached or error occurred. Unable to get a response.\")\n",
        "    return None"
      ],
      "metadata": {
        "id": "DCNEHjHvlGj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_statistics(query):\n",
        "    response = []\n",
        "\n",
        "    # Normalize the query to lowercase\n",
        "    normalized_query = query.lower()\n",
        "\n",
        "    # Define search terms for each dataset\n",
        "    search_terms = {\n",
        "        'ipc crimes 2019': ipc_crimes_df,\n",
        "        'crime foreigners 2022': crime_foreigners_2022_df,\n",
        "        'crime foreigners 2017 2021': crime_foreigners_2017_2021_df\n",
        "    }\n",
        "\n",
        "    # Check each dataset\n",
        "    for term, df in search_terms.items():\n",
        "        if term in normalized_query:\n",
        "            # Check if the query matches any of the rows\n",
        "            for index, row in df.iterrows():\n",
        "                # Use str.contains for more efficient search\n",
        "                if row.astype(str).str.contains(normalized_query).any():\n",
        "                    response.append(row.to_string())\n",
        "\n",
        "    return \"\\n\".join(response) if response else \"I couldn't find any statistics related to your query.\"\n"
      ],
      "metadata": {
        "id": "gS7dc_RAvcKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flag = True\n",
        "print(\"ROBO: My name is Robo. Let's have a conversation! Also, if you want to exit any time, just type Bye!\")\n",
        "\n",
        "while flag:\n",
        "    user_response = input()\n",
        "    user_response = user_response.lower()\n",
        "    if user_response != 'bye':\n",
        "        if user_response == 'thanks' or user_response == 'thank you':\n",
        "            flag = False\n",
        "            print(\"ROBO: You are welcome..\")\n",
        "        else:\n",
        "            if greet(user_response) is not None:\n",
        "                print(\"ROBO: \" + greet(user_response))\n",
        "            else:\n",
        "                sent_tokens.append(user_response)\n",
        "                word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
        "                final_words = list(set(word_tokens))\n",
        "                print(\"ROBO: \", end=\"\")\n",
        "                bot_response = response(user_response)\n",
        "                if \"I am sorry! I don't understand you\" in bot_response:\n",
        "                    # Try to fetch statistics first\n",
        "                    stats_response = fetch_statistics(user_response)\n",
        "                    if stats_response == \"I couldn't find any statistics related to your query.\":\n",
        "                        # If no statistics found, use ChatGPT response\n",
        "                        bot_response = chatgpt_response(user_response)\n",
        "                    else:\n",
        "                        bot_response = stats_response\n",
        "                print(bot_response)\n",
        "                sent_tokens.remove(user_response)\n",
        "    else:\n",
        "        flag = False\n",
        "        print(\"ROBO: Bye! take care..\")"
      ],
      "metadata": {
        "id": "NpRNtysTedFr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}